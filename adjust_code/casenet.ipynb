{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a6beb5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-26T04:44:52.390971Z",
     "iopub.status.busy": "2024-10-26T04:44:52.390612Z",
     "iopub.status.idle": "2024-10-26T04:45:05.635653Z",
     "shell.execute_reply": "2024-10-26T04:45:05.634554Z"
    },
    "papermill": {
     "duration": 13.251638,
     "end_time": "2024-10-26T04:45:05.638193",
     "exception": false,
     "start_time": "2024-10-26T04:44:52.386555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\r\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\r\n",
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.18.3)\r\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\r\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\r\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.43)\r\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (3.11.0)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\r\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.2)\r\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.32.3)\r\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.15.0)\r\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (70.0.0)\r\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\r\n",
      "Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\r\n",
      "Installing collected packages: torchsummary\r\n",
      "Successfully installed torchsummary-1.5.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326502b1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-26T04:45:05.645955Z",
     "iopub.status.busy": "2024-10-26T04:45:05.645162Z",
     "iopub.status.idle": "2024-10-26T09:07:49.392826Z",
     "shell.execute_reply": "2024-10-26T09:07:49.391739Z"
    },
    "papermill": {
     "duration": 15763.75404,
     "end_time": "2024-10-26T09:07:49.395246",
     "exception": false,
     "start_time": "2024-10-26T04:45:05.641206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mseitomoyi\u001b[0m (\u001b[33mseitomoyi-carnegie-mellon-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20241026_044513-xt86nszx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgenerous-wildflower-4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/seitomoyi-carnegie-mellon-university/CASENet\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/seitomoyi-carnegie-mellon-university/CASENet/runs/xt86nszx\u001b[0m\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97.8M/97.8M [00:00<00:00, 188MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 128, 128]           3,136\n",
      "       BatchNorm2d-2         [-1, 64, 128, 128]             128\n",
      "              ReLU-3         [-1, 64, 128, 128]               0\n",
      "         MaxPool2d-4           [-1, 64, 64, 64]               0\n",
      "            Conv2d-5           [-1, 64, 64, 64]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 64, 64]             128\n",
      "              ReLU-7           [-1, 64, 64, 64]               0\n",
      "            Conv2d-8           [-1, 64, 64, 64]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 64, 64]             128\n",
      "             ReLU-10           [-1, 64, 64, 64]               0\n",
      "           Conv2d-11          [-1, 256, 64, 64]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 64, 64]             512\n",
      "           Conv2d-13          [-1, 256, 64, 64]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 64, 64]             512\n",
      "             ReLU-15          [-1, 256, 64, 64]               0\n",
      "       Bottleneck-16          [-1, 256, 64, 64]               0\n",
      "           Conv2d-17           [-1, 64, 64, 64]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 64, 64]             128\n",
      "             ReLU-19           [-1, 64, 64, 64]               0\n",
      "           Conv2d-20           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 64, 64]             128\n",
      "             ReLU-22           [-1, 64, 64, 64]               0\n",
      "           Conv2d-23          [-1, 256, 64, 64]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 64, 64]             512\n",
      "             ReLU-25          [-1, 256, 64, 64]               0\n",
      "       Bottleneck-26          [-1, 256, 64, 64]               0\n",
      "           Conv2d-27           [-1, 64, 64, 64]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 64, 64]             128\n",
      "             ReLU-29           [-1, 64, 64, 64]               0\n",
      "           Conv2d-30           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 64, 64]             128\n",
      "             ReLU-32           [-1, 64, 64, 64]               0\n",
      "           Conv2d-33          [-1, 256, 64, 64]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 64, 64]             512\n",
      "             ReLU-35          [-1, 256, 64, 64]               0\n",
      "       Bottleneck-36          [-1, 256, 64, 64]               0\n",
      "           Conv2d-37          [-1, 128, 64, 64]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 64, 64]             256\n",
      "             ReLU-39          [-1, 128, 64, 64]               0\n",
      "           Conv2d-40          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 32, 32]             256\n",
      "             ReLU-42          [-1, 128, 32, 32]               0\n",
      "           Conv2d-43          [-1, 512, 32, 32]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 32, 32]           1,024\n",
      "           Conv2d-45          [-1, 512, 32, 32]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-47          [-1, 512, 32, 32]               0\n",
      "       Bottleneck-48          [-1, 512, 32, 32]               0\n",
      "           Conv2d-49          [-1, 128, 32, 32]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 32, 32]             256\n",
      "             ReLU-51          [-1, 128, 32, 32]               0\n",
      "           Conv2d-52          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 32, 32]             256\n",
      "             ReLU-54          [-1, 128, 32, 32]               0\n",
      "           Conv2d-55          [-1, 512, 32, 32]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-57          [-1, 512, 32, 32]               0\n",
      "       Bottleneck-58          [-1, 512, 32, 32]               0\n",
      "           Conv2d-59          [-1, 128, 32, 32]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 32, 32]             256\n",
      "             ReLU-61          [-1, 128, 32, 32]               0\n",
      "           Conv2d-62          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 32, 32]             256\n",
      "             ReLU-64          [-1, 128, 32, 32]               0\n",
      "           Conv2d-65          [-1, 512, 32, 32]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-67          [-1, 512, 32, 32]               0\n",
      "       Bottleneck-68          [-1, 512, 32, 32]               0\n",
      "           Conv2d-69          [-1, 128, 32, 32]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 32, 32]             256\n",
      "             ReLU-71          [-1, 128, 32, 32]               0\n",
      "           Conv2d-72          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 32, 32]             256\n",
      "             ReLU-74          [-1, 128, 32, 32]               0\n",
      "           Conv2d-75          [-1, 512, 32, 32]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-77          [-1, 512, 32, 32]               0\n",
      "       Bottleneck-78          [-1, 512, 32, 32]               0\n",
      "           Conv2d-79          [-1, 256, 32, 32]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 32, 32]             512\n",
      "             ReLU-81          [-1, 256, 32, 32]               0\n",
      "           Conv2d-82          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 16, 16]             512\n",
      "             ReLU-84          [-1, 256, 16, 16]               0\n",
      "           Conv2d-85         [-1, 1024, 16, 16]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 16, 16]           2,048\n",
      "           Conv2d-87         [-1, 1024, 16, 16]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 16, 16]           2,048\n",
      "             ReLU-89         [-1, 1024, 16, 16]               0\n",
      "       Bottleneck-90         [-1, 1024, 16, 16]               0\n",
      "           Conv2d-91          [-1, 256, 16, 16]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 16, 16]             512\n",
      "             ReLU-93          [-1, 256, 16, 16]               0\n",
      "           Conv2d-94          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 16, 16]             512\n",
      "             ReLU-96          [-1, 256, 16, 16]               0\n",
      "           Conv2d-97         [-1, 1024, 16, 16]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 16, 16]           2,048\n",
      "             ReLU-99         [-1, 1024, 16, 16]               0\n",
      "      Bottleneck-100         [-1, 1024, 16, 16]               0\n",
      "          Conv2d-101          [-1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 16, 16]             512\n",
      "            ReLU-103          [-1, 256, 16, 16]               0\n",
      "          Conv2d-104          [-1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 16, 16]             512\n",
      "            ReLU-106          [-1, 256, 16, 16]               0\n",
      "          Conv2d-107         [-1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 16, 16]           2,048\n",
      "            ReLU-109         [-1, 1024, 16, 16]               0\n",
      "      Bottleneck-110         [-1, 1024, 16, 16]               0\n",
      "          Conv2d-111          [-1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 16, 16]             512\n",
      "            ReLU-113          [-1, 256, 16, 16]               0\n",
      "          Conv2d-114          [-1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 16, 16]             512\n",
      "            ReLU-116          [-1, 256, 16, 16]               0\n",
      "          Conv2d-117         [-1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 16, 16]           2,048\n",
      "            ReLU-119         [-1, 1024, 16, 16]               0\n",
      "      Bottleneck-120         [-1, 1024, 16, 16]               0\n",
      "          Conv2d-121          [-1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 16, 16]             512\n",
      "            ReLU-123          [-1, 256, 16, 16]               0\n",
      "          Conv2d-124          [-1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 16, 16]             512\n",
      "            ReLU-126          [-1, 256, 16, 16]               0\n",
      "          Conv2d-127         [-1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 16, 16]           2,048\n",
      "            ReLU-129         [-1, 1024, 16, 16]               0\n",
      "      Bottleneck-130         [-1, 1024, 16, 16]               0\n",
      "          Conv2d-131          [-1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 16, 16]             512\n",
      "            ReLU-133          [-1, 256, 16, 16]               0\n",
      "          Conv2d-134          [-1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 16, 16]             512\n",
      "            ReLU-136          [-1, 256, 16, 16]               0\n",
      "          Conv2d-137         [-1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 16, 16]           2,048\n",
      "            ReLU-139         [-1, 1024, 16, 16]               0\n",
      "      Bottleneck-140         [-1, 1024, 16, 16]               0\n",
      "          Conv2d-141          [-1, 512, 16, 16]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-143          [-1, 512, 16, 16]               0\n",
      "          Conv2d-144            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-146            [-1, 512, 8, 8]               0\n",
      "          Conv2d-147           [-1, 2048, 8, 8]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 8, 8]           4,096\n",
      "          Conv2d-149           [-1, 2048, 8, 8]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 8, 8]           4,096\n",
      "            ReLU-151           [-1, 2048, 8, 8]               0\n",
      "      Bottleneck-152           [-1, 2048, 8, 8]               0\n",
      "          Conv2d-153            [-1, 512, 8, 8]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-155            [-1, 512, 8, 8]               0\n",
      "          Conv2d-156            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-158            [-1, 512, 8, 8]               0\n",
      "          Conv2d-159           [-1, 2048, 8, 8]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 8, 8]           4,096\n",
      "            ReLU-161           [-1, 2048, 8, 8]               0\n",
      "      Bottleneck-162           [-1, 2048, 8, 8]               0\n",
      "          Conv2d-163            [-1, 512, 8, 8]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-165            [-1, 512, 8, 8]               0\n",
      "          Conv2d-166            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-168            [-1, 512, 8, 8]               0\n",
      "          Conv2d-169           [-1, 2048, 8, 8]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 8, 8]           4,096\n",
      "            ReLU-171           [-1, 2048, 8, 8]               0\n",
      "      Bottleneck-172           [-1, 2048, 8, 8]               0\n",
      "         BaseNet-173           [-1, 2048, 8, 8]               0\n",
      "          Conv2d-174              [-1, 1, 8, 8]           2,049\n",
      "GrayscaleSegmentationHead-175          [-1, 1, 256, 256]               0\n",
      "================================================================\n",
      "Total params: 23,503,809\n",
      "Trainable params: 23,503,809\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 375.75\n",
      "Params size (MB): 89.66\n",
      "Estimated Total Size (MB): 465.66\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 1: 0.121132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 2: 0.088650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 3: 0.080725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 4: 0.076005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 5: 0.076377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 6: 0.066081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 7: 0.063836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 8: 0.060089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 9: 0.052383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 10: 0.050044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 11: 0.042522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 12: 0.037193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 13: 0.034212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 14: 0.029344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 15: 0.026449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 16: 0.022783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 17: 0.020375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 18: 0.019138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 19: 0.019022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 20: 0.018863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 21: 0.020086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 22: 0.019523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 23: 0.016010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 24: 0.014061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 25: 0.011982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 26: 0.011407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 27: 0.010282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 28: 0.011862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 29: 0.031071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 30: 0.025613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 31: 0.016086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 32: 0.011416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 33: 0.010063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 34: 0.009052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 35: 0.007985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 36: 0.007651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 37: 0.007861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 38: 0.007541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 39: 0.007241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 40: 0.006667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 41: 0.006807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 42: 0.007776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 43: 0.008168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 44: 0.034171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 45: 0.029987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 46: 0.015173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 47: 0.010269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 48: 0.007982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 49: 0.006843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 50: 0.006285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 51: 0.005986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 52: 0.006022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 53: 0.005772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 54: 0.005600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 55: 0.005187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 56: 0.005269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 57: 0.005138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 58: 0.005141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 59: 0.005050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 60: 0.004904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 61: 0.005492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 62: 0.005289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 63: 0.046071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 64: 0.023580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 65: 0.011983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 66: 0.007962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 67: 0.006235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 68: 0.005496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 69: 0.005279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 70: 0.005195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 71: 0.004950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 72: 0.004817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 73: 0.004967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 74: 0.004733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 75: 0.004595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 76: 0.007537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 77: 0.026396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 78: 0.013720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 79: 0.010611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 80: 0.006515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 81: 0.005331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 82: 0.004928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 83: 0.004612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 84: 0.004558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 85: 0.004439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 86: 0.004285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 87: 0.004593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 88: 0.004561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 89: 0.004382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 90: 0.004437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 91: 0.025424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 92: 0.014293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 93: 0.008125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 94: 0.005938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 95: 0.004891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 96: 0.004427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 97: 0.004396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 98: 0.004291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 99: 0.004262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss after epoch 100: 0.004275\n",
      "Model saved as segmentation_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "import wandb\n",
    "\n",
    "\n",
    "config = {\n",
    "    'batch_size': 64,\n",
    "    'lr': 0.001,\n",
    "    'epochs': 100,\n",
    "    'data_dir': \"/kaggle/input/mydataset/dataset/EM_processed/training\",\n",
    "    'data_label_dir': \"/kaggle/input/mydataset/dataset/Label_processed/training\",\n",
    "    'checkpoint_dir': \"/kaggle/working/\",\n",
    "#     'val_pair_dir': \"/content/data/11-785-f24-hw2p2-verification/val_pairs.txt\",\n",
    "#     'test_pair_dir': \"/content/data/11-785-f24-hw2p2-verification/test_pairs.txt\"\n",
    "}\n",
    "    \n",
    "    \n",
    "wandb.login(key=\"98642f33baa53793f08e5f32f1d09da8c7c6e80b\")\n",
    "run = wandb.init(\n",
    "    reinit=True,\n",
    "    project=\"CASENet\",\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Custom Dataset Class\n",
    "class BWImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None):\n",
    "        super(BWImageDataset, self).__init__()\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_names = sorted(os.listdir(image_dir))\n",
    "        self.label_names = sorted(os.listdir(label_dir))\n",
    "        assert len(self.image_names) == len(self.label_names), \"Mismatch between images and labels\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and label\n",
    "        image = Image.open(os.path.join(self.image_dir, self.image_names[idx])).convert('L')\n",
    "        label = Image.open(os.path.join(self.label_dir, self.label_names[idx])).convert('L')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            label = self.transform(label)\n",
    "        else:\n",
    "            image = transforms.ToTensor()(image)\n",
    "            label = transforms.ToTensor()(label)\n",
    "\n",
    "        # Normalize label to be in [0,1]\n",
    "        label = (label > 0).float()\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Function to modify ResNet to accept single-channel input\n",
    "def get_resnet_backbone(backbone_name='resnet50', pretrained=True):\n",
    "    if backbone_name == 'resnet50':\n",
    "        backbone = models.resnet50(pretrained=pretrained)\n",
    "    elif backbone_name == 'resnet101':\n",
    "        backbone = models.resnet101(pretrained=pretrained)\n",
    "    else:\n",
    "        raise ValueError('Unsupported backbone {}'.format(backbone_name))\n",
    "\n",
    "    # Modify the first convolution layer to accept 1-channel input\n",
    "    backbone.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    # If pretrained, adjust weights (optional)\n",
    "    if pretrained:\n",
    "        original_conv1 = models.resnet50(pretrained=True).conv1\n",
    "        backbone.conv1.weight.data = original_conv1.weight.data.sum(dim=1, keepdim=True)\n",
    "    return backbone\n",
    "\n",
    "# BaseNet Class (modified to accept 1-channel input)\n",
    "class BaseNet(nn.Module):\n",
    "    def __init__(self, backbone_name='resnet50'):\n",
    "        super(BaseNet, self).__init__()\n",
    "        self.backbone = get_resnet_backbone(backbone_name, pretrained=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward through ResNet backbone\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Segmentation Head for Grayscale Output\n",
    "class GrayscaleSegmentationHead(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(GrayscaleSegmentationHead, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        # Upsample to match input size (assuming input size divisible by 32)\n",
    "        x = nn.functional.interpolate(x, scale_factor=32, mode='bilinear', align_corners=False)\n",
    "        # Output is continuous, no activation here since BCEWithLogitsLoss expects logits\n",
    "        return x\n",
    "\n",
    "# Complete Model\n",
    "class SegmentationModel(nn.Module):\n",
    "    def __init__(self, backbone_name='resnet50'):\n",
    "        super(SegmentationModel, self).__init__()\n",
    "        self.base_net = BaseNet(backbone_name)\n",
    "        self.seg_head = GrayscaleSegmentationHead(2048)  # ResNet-50 outputs 2048 features\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_net(x)\n",
    "        x = self.seg_head(x)\n",
    "        return x\n",
    "\n",
    "# Training Function\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch}', leave=False)\n",
    "    \n",
    "    for batch_idx, (data, target) in progress_bar:\n",
    "        data = data.to(device, dtype=torch.float32)\n",
    "        target = target.to(device, dtype=torch.float32)  # For BCEWithLogitsLoss, target should be float\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # Output shape: [B, 1, H, W], Target shape: [B, 1, H, W]\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        metrics = {\n",
    "            'train_loss': loss,\n",
    "#             'val_loss': val_loss,\n",
    "        }\n",
    "        \n",
    "        run.log(metrics)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    print('Training Loss after epoch {}: {:.6f}'.format(epoch, running_loss / len(train_loader)))\n",
    "\n",
    "\n",
    "# Main Function\n",
    "def main():\n",
    "    # Transforms\n",
    "    transform = transforms.Compose([\n",
    "        # transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Dataset and DataLoader\n",
    "    train_dataset = BWImageDataset(config['data_dir'], config['data_label_dir'], transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "\n",
    "    # Model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = SegmentationModel(backbone_name='resnet50').to(device)\n",
    "\n",
    "    summary(model,(1,256,256))\n",
    "\n",
    "    # Optimizer and Loss Function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(1, config['epochs'] + 1):\n",
    "        train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), os.path.join(config['checkpoint_dir'],'segmentation_model.pth'))\n",
    "    print('Model saved as segmentation_model.pth')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5950318,
     "sourceId": 9724669,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15785.161938,
   "end_time": "2024-10-26T09:07:54.491827",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-26T04:44:49.329889",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
